# Critical packages
import datetime as dt
import numpy as np
from scipy.special import comb
import matplotlib
matplotlib.use("Agg")
from matplotlib.backends.backend_pdf import PdfPages
import matplotlib.pyplot as plt

# Functions from other modules
from define import qcode as qec
import define.globalvars as gv
from define.fnames import NRWeightsFile, NRWeightsPlotFile


def ComputeNRBudget(nr_weights_all, alphas, nq):
	# Compute the relative budget of weight-w error rates in the NR dataset.
	n_paulis = nr_weights_all.size
	max_weight = np.max(nr_weights_all)
	n_alphas = alphas.size
	relative_budget = np.zeros((n_alphas, max_weight + 1), dtype=np.float)
	
	for (alpha_count, alpha) in enumerate(alphas):
		budget_pauli_count = max(int(alpha * n_paulis),1)
		nr_weights = nr_weights_all[ : budget_pauli_count]
		weights, weight_count = np.unique(nr_weights, return_counts = True)
		# print("alpha = {}\nWeights : {} and their frequencies : {} ".format(alpha, weights, weight_count))
		
		# Resetting the number of errors of each weight to the theoretical maximum
		nerrors_weight = np.zeros(max_weight + 1, dtype = np.int)
		excess_budget = 0
		for (i, w) in enumerate(weights):
			count = weight_count[i]
			if(count > comb(nq, w) * (3 ** w)):
				nerrors_weight[w] = comb(nq, w) * (3 ** w)
				excess_budget += count - nerrors_weight[w]
			else:
				nerrors_weight[w] = count
		
		# Redistributing excess errors generated by Poisson distribution
		# Lower weights get priority over higher ones
		for w in range(max_weight + 1):
			need_weight_w = comb(nq, w)*(3 ** w) - nerrors_weight[w]
			add_to_weight_w = min(excess_budget, need_weight_w)
			nerrors_weight[w] += add_to_weight_w
			excess_budget -= add_to_weight_w
		relative_budget[alpha_count, :] = [nerrors_weight[w]*100/budget_pauli_count for w in range(max_weight+1)]
	
	return relative_budget


def NRWeightsPlot(dbses, noise, sample):
	# Compute the relative budget taken up by the set of Pauli error rates for each weight, in the NR dataset.
	# Plot histograms one on top of each other: stacked histogram.
	nq = dbses[0].eccs[0].N
	nr_weights = np.loadtxt(NRWeightsFile(dbs, noise))[sample, :]
	alphas = np.array([dbs.dcfraction for dbs in dbses], dtype = np.float)
	percentages = ComputeNRBudget(nr_weights, alphas, nq)
	(n_rows, n_cols) = percentages.shape
	
	plotfname = NRWeightsPlotFile(dbses[0], phymet, logmet)
	with PdfPages(plotfname) as pdf:
		fig = plt.figure(figsize=gv.canvas_size)

		# We want the histograms for each weight, stacked vertically. So we need to compute the bottom of each bar.
		bottoms = np.zeros(n_rows)
		for w in range(n_cols):
			plt.bar(np.arange(n_rows), percentages[:, w], width = 0.7, bottom = bottoms, label = "w = %d" % (w), color=gv.Colors[w % gv.n_Colors])
			bottoms += percentages[:, w]
		
		plt.ylabel("Relative budget", fontsize=gv.axes_labels_fontsize)
		plt.xlabel("$\\alpha$", fontsize=gv.axes_labels_fontsize)
		
		# plt.title('Weight wise distribution of NR data')
		# plt.xticks(ind[::5], np.round(alphas,4)[::5], rotation = 45)
		
		plt.xticks(np.arange(n_rows), np.round(alphas, 4), rotation = 45)
		ax.tick_params(axis="both", which="both", pad=gv.ticks_pad, direction="inout", length=gv.ticks_length, width=gv.ticks_width, labelsize=gv.ticks_fontsize)

		# plt.yticks(np.arange(0, 100, 25))

		# Legend
		ax.legend(numpoints=1, loc=1, shadow=True, fontsize=gv.legend_fontsize, markerscale=gv.legend_marker_scale)
		
		# Save the plot
		pdf.savefig(fig)
		plt.close()
		
		# Set PDF attributes
		pdfInfo = pdf.infodict()
		pdfInfo["Title"] = "Pauli distribution of errors."
		pdfInfo["ModDate"] = dt.datetime.today()

	return None