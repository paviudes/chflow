import numpy as np
from define.QECCLfid import utils as ut
from define.randchans import CreateIIDPauli, ReconstructPauliChannel
from define.chanreps import PauliConvertToTransfer
from define import fnames as fn
from scipy.special import comb
from define.qcode import PrepareSyndromeLookUp
from define.QECCLfid.utils import SamplePoisson

def SetDecoderKnowledge(submit, rawchan=None, noise=None, sample=None):
	# Set the decoder knowledge variable for the submission object.
	nr_weights = None
	if ((submit.decoders)[0] == 3 or (submit.decoders)[0] == 4): # Introduced decoder 4 to capture top alpha grouped by weight
		if submit.iscorr == 0:
			chan_probs = np.tile(
				np.real(np.diag(ConvertRepresentations(physical, "process", "chi"))),
				[submit.eccs[0].N, 1],
			)
		elif submit.iscorr == 2:
			chans_ptm = np.reshape(physical, [submit.eccs[0].N, 4, 4])
			chan_probs = np.zeros((submit.eccs[0].N, 4), dtype=np.double)
			for q in range(submit.eccs[0].N):
				chan_probs[q, :] = np.real(
					np.diag(
						ConvertRepresentations(chans_ptm[q, :, :], "process", "chi")
					)
				)
		else:
			chan_probs = rawchan
		if (submit.decoders)[0] == 3:
			mpinfo = CompleteDecoderKnowledge(submit.decoder_fraction, chan_probs, submit.eccs[0], option="full", nr_weights = None).astype(np.float64)
		else:
			# decoder is 4 i.e distribute by weight guided by Poisson
			# print("noise = {}, sample = {}".format(noise, sample))
			nr_weights = np.load(fn.NRWeightsFile(submit, noise))[sample, :]
			mpinfo = CompleteDecoderKnowledge(submit.decoder_fraction, chan_probs, submit.eccs[0], option="weight", nr_weights = nr_weights).astype(np.float64)
	else:
		mpinfo = np.zeros(4**submit.eccs[0].N, dtype=np.float64)
	return (mpinfo, nr_weights)


def TailorDecoder(qecc, channel, levels, bias=None):
	# Tailor a decoder to an error model by exploiting simple structure.
	# At the moment, this only works differently from MWD for a biased Pauli error model "bpauli".
	# We need to design the relative importance that should be given to I, X, Y and Z errors.
	# print("TailorDecoder({}, {})".format(submit.channel, noise))
	if (channel in ["bpauli", "crsum"]):
		cX = int(bias)
		cZ = 1
		cY = int(bias)
		qecc.weight_convention = {"method": "bias", "weights": {"X": cX, "Y": cY, "Z": cZ}}
		PrepareSyndromeLookUp(qecc)
		# print("Lookup table for {} code with bias {}.".format(qecc.name, bias))
	else:
		for l in range(levels):
			qecc.weight_convention = {"method": "Hamming"}
			PrepareSyndromeLookUp(qecc)
	return None

def GetTotalErrorBudget(dbs, noise, sample):
	# Compute the total number of distinct Pauli error rates included in the NR dataset.
	nrw = np.load(fn.NRWeightsFile(dbs, noise))[sample, :]
	max_weight = 1 + dbs.eccs[0].N//2
	(weight_count_alpha, __) = ComputeNRBudget(nrw, [dbs.decoder_fraction], dbs.eccs[0].N, max_weight=max_weight)
	budget = np.sum(weight_count_alpha)
	# print("alpha = {}, budget = {}".format(dbs.decoder_fraction, budget))
	return budget


def ComputeNRBudget(nr_weights_all, alphas, nq, max_weight=None):
	# Compute the relative budget of weight-w error rates in the NR dataset.
	n_paulis = nr_weights_all.size
	if max_weight is None:
		max_weight = nr_weights_all.max()
	n_alphas = len(alphas)
	relative_budget = np.zeros((n_alphas, max_weight + 1), dtype=np.float64)

	# Count the frequency of each weight.
	weight_counts = np.zeros(max_weight + 1, dtype=np.int64)
	min_weight_counts = np.zeros(max_weight + 1, dtype=np.int64)
	for w in range(max_weight + 1):
		weight_counts[w] = np.count_nonzero(nr_weights_all == w)
	min_weight_counts[0] = 1

	for (alpha_count, alpha) in enumerate(alphas):
		weight_count_alpha = np.maximum((alpha * weight_counts).astype(np.int64), min_weight_counts)
		budget_pauli_count = np.sum(weight_count_alpha)
		# print("alpha = {}, budget_pauli_count = {}\nweight_count_alpha\n{}".format(alpha, budget_pauli_count, weight_count_alpha))

		# Resetting the number of errors of each weight to the theoretical maximum
		nerrors_weight = np.zeros(max_weight + 1, dtype = np.int64)
		excess_budget = 0
		for w in range(weight_counts.size):
			count = weight_count_alpha[w]
			if(count > comb(nq, w) * (3 ** w)):
				nerrors_weight[w] = comb(nq, w) * (3 ** w)
				excess_budget += count - nerrors_weight[w]
			else:
				nerrors_weight[w] = count

		# Redistributing excess errors generated by Poisson distribution
		# Lower weights get priority over higher ones
		for w in range(max_weight + 1):
			need_weight_w = comb(nq, w)*(3 ** w) - nerrors_weight[w]
			add_to_weight_w = min(excess_budget, need_weight_w)
			nerrors_weight[w] += add_to_weight_w
			excess_budget -= add_to_weight_w

		relative_budget[alpha_count, :] = [nerrors_weight[w]*100/budget_pauli_count for w in range(max_weight+1)]

	return (nerrors_weight, relative_budget)


def GetLeadingPaulis(lead_frac, qcode, chan_probs, option, nr_weights_all = None, max_weight = None):
	# Get the leading Pauli probabilities in the iid model.
	# To get the indices of the k-largest elements: https://stackoverflow.com/questions/6910641/how-do-i-get-indices-of-n-maximum-values-in-a-numpy-array
	# If the option is "full", it supplies top alpha fraction of entire chi diagonal
	# If the option is "weight", it supplies errors with weights sampled from Poisson distribution having mean 1 and excess budget redistributed
	if chan_probs.ndim > 1:
		chan_probs = ut.GetErrorProbabilities(qcode.PauliOperatorsLST, chan_probs, 0)

	if (option == "full"):
		nPaulis = max(1, int(lead_frac * (4 ** qcode.N)))
		leading_paulis = np.argsort(chan_probs)[-nPaulis:][::-1]

	elif (option == "sqprobs"):
		single_qubit_errors = np.concatenate((np.array([0], dtype = np.int64), qcode.group_by_weight[1]))
		nPaulis = max(1, int(lead_frac * (4 ** qcode.N)))
		top_paulis = np.argsort(chan_probs)[-nPaulis:][::-1]
		remaining_budget = 0
		if (nPaulis > (3 * qcode.N + 1)):
			remaining_budget = nPaulis - (3 * qcode.N + 1)

		# Append single qubit errors to leading_probs
		leading_paulis = np.concatenate((single_qubit_errors, top_paulis))
		(__, inds) = np.unique(leading_paulis, return_index=True)
		leading_paulis = leading_paulis[np.sort(inds)][ : ((3 * qcode.N + 1) + remaining_budget)]
		

	elif option == "weight":

		if max_weight is None:
			max_weight = qcode.N//2 + 1

		if qcode.group_by_weight is None:
			PrepareSyndromeLookUp(qcode)

		(nerrors_weight, __) = ComputeNRBudget(nr_weights_all, [lead_frac], qcode.N, max_weight=max_weight)

		leading_paulis = np.zeros(np.sum(nerrors_weight, dtype = np.int64), dtype=np.int64)
		start = 0
		for w in range(max_weight + 1):
			if (nerrors_weight[w] > 0):
				stop = start + nerrors_weight[w]
				errors_wtw = qcode.group_by_weight[w]
				indices_picked = errors_wtw[np.argsort(chan_probs[errors_wtw])[-nerrors_weight[w]:]]
				leading_paulis[start:stop] = indices_picked[:]
				start = stop
	else:
		pass

	"""
	##### Print cases where weight 2 errors are found in the leading alpha.
	if (lead_frac == 0.0014):
		if not np.all(np.in1d(leading_paulis, np.concatenate((qcode.group_by_weight[0], qcode.group_by_weight[1])))):
			print("Weight - 1 violation.\n")
			print("leading_paulis\n{}\nweight 1 errors:\n{}".format(np.sort(leading_paulis), np.sort(np.concatenate((qcode.group_by_weight[0], qcode.group_by_weight[1])))))
	elif (lead_frac == 0.013):
		if not np.all(np.in1d(np.sort(leading_paulis), np.sort(np.concatenate((qcode.group_by_weight[0], qcode.group_by_weight[1], qcodes[0].group_by_weight[2]))))):
			print("Weight - 2 violation.")
	else:
		pass
	#####
	"""

	return (1 - chan_probs[0], leading_paulis, chan_probs[leading_paulis])


def CompleteDecoderKnowledge(leading_fraction, chan_probs, qcode, option = "full", nr_weights = None):
	# Complete the probabilities given to a ML decoder.
	(infid, known_paulis, known_probs) = GetLeadingPaulis(leading_fraction, qcode, chan_probs, option, nr_weights)
	"""
	Create a function similar to GetLeadingPaulis.
	This function will simply identify the Pauli indices (in LST) we need to keep from NR.
	"""
	# print(
	#     "Number of known paulis in decoder knowledge = {}".format(known_paulis.shape[0])
	# )
	# print("Total known probability = {}".format(np.sum(known_probs)))

	if ((option == "full") or (option == "weight")):
		infid_qubit = 1 - np.power(1 - infid, 1 / qcode.N)
		# depolarizing_rate = infid_qubit # If noise is non-unitary
		depolarizing_rate = np.sqrt(infid_qubit) # If noise is unitary
		depolarizing_rate = np.power(depolarizing_rate, 0.8) # If the decoder is correlation aware.
		decoder_probs = CreateIIDPauli(depolarizing_rate, qcode) 
		
	elif (option == "sqprobs"):
		if chan_probs.ndim > 1:
			pauli_probs = ut.GetErrorProbabilities(qcode.PauliOperatorsLST, chan_probs, 0)
		else:
			pauli_probs = chan_probs
		decoder_probs = ReconstructPauliChannel(pauli_probs, qcode)

	else:
		pass

	# print("RAW Decoder ansatz before normalization\n{}".format(np.sort(decoder_probs)[::-1][:30]))
	decoder_probs[known_paulis] = known_probs
	total_unknown = 1 - np.sum(known_probs)
	# print("Total unknown probability = {}".format(total_unknown))
	# Normalize the unknown Paulis
	# https://stackoverflow.com/questions/27824075/accessing-numpy-array-elements-not-in-a-given-index-list
	mask = np.ones(decoder_probs.shape[0], dtype=bool)
	mask[known_paulis] = False
	decoder_probs[mask] = total_unknown * decoder_probs[mask] / np.sum(decoder_probs[mask])
	# print("RAW Decoder ansatz after normalization\n{}".format(np.sort(decoder_probs)[::-1][:30]))
	# print("Sum of decoder probs = {}".format(np.sum(decoder_probs)))
	# print("True NR data\n{}".format(np.sort(chan_probs)[::-1][:30]))
	# print("Known paulis weights\n{}\nNR data Known probs\n{}".format(qcode.weightdist[known_paulis], known_probs))
	# print("Decoder ansatz\n{}".format(np.sort(decoder_probs)[::-1][:30]))
	# print("xxxxxxxxxxxxxxxxx")
	return decoder_probs


def PrepareNRWeights(submit):
	# Prepare the weights of Pauli errors that will be supplied to the decoder: nr_weights.
	# Use properties of submit to retrieve the mean and cutoff of \the Poisson distribution: submit.noiserates[i, :] = (__, cutoff, __, mean)
	# Save the nr_weights to a file.
	qcode = submit.eccs[0]
	max_weight = qcode.N//2 + 1
	submit.nr_weights = np.zeros((submit.noiserates.shape[0], submit.samps, 4 ** qcode.N), dtype = np.int64)
	for r in range(submit.noiserates.shape[0]):
		if (submit.channel == "cptp"):
			(__, cutoff, __, mean) = submit.noiserates[r, :]
		else:
			mean = 1
			cutoff = max_weight
		for s in range(submit.samps):
			submit.nr_weights[r, s, :] = [SamplePoisson(mean, cutoff=max_weight) for __ in range(4 ** qcode.N)]
	return None


def PrepareChannelDecoder(submit, noise, sample):
	# Generate the reference channel for partial ML decoder
	# Reference channel at present is only created for level 1
	l = 0
	qcode = submit.eccs[l]
	(nrows, ncols) = (4 ** submit.eccs[l].K, 4 ** submit.eccs[l].K)
	diagmask = [
		q * ncols * nrows + ncols * j + j for q in range(qcode.N) for j in range(nrows)
	]
	rawchan = np.load(fn.RawPhysicalChannel(submit, noise))[sample, :]
	if submit.iscorr == 0:
		chan_probs = np.reshape(np.tile(rawchan, qcode.N)[diagmask], [qcode.N, nrows])
	elif submit.iscorr == 2:
		chan_probs = np.reshape(rawchan[diagmask], [qcode.N, nrows])
	else:
		chan_probs = rawchan
	decoder_probs = CompleteDecoderKnowledge(submit.decoder_fraction, chan_probs, qcode)
	decoder_knowledge = PauliConvertToTransfer(decoder_probs, qcode)
	return decoder_knowledge
